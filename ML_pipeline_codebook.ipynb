{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "formed-potential",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. [TFX](#TFX)\n",
    "2. [Apache Beam](#Apache-Beam)\n",
    "3. [Data Ingestion](#Data-Ingestion)\n",
    "    * [Data Loading](#Data-Loading)\n",
    "    * [Data Preparation](#Data-Preparation)\n",
    "    * [Versioning Datasets](#Versioning-Datasets)\n",
    "    * [Ingestion Strategies](#Ingestion-Strategies)\n",
    "\n",
    "4. [Data Validation](#Data-Validation)\n",
    "    * [Data Validation](#Data-Validation)\n",
    "    * [Processing Large Datasets with GCP](#Processing-Large-Datasets-with-GCP)\n",
    "    * [Integrating TFDV into Your ML Pipeline](#Integrating-TFDV-into-Your-ML-Pipeline)\n",
    "5. [Data Preprocessing with TFT](#Data-Preprocessing-with-TFT)\n",
    "6. [Integrate TFT into Your Machine Learning Pipeline](#Integrate-TFT-into-Your-Machine-Learning-Pipeline)\n",
    "\n",
    "7. [Model training](#Model-training)\n",
    "    * [The TFX Trainer Component](#The-TFX-Trainer-Component)\n",
    "    \n",
    "8. [Model analysis and validation](#Model-analysis-and-validation)\n",
    "9. [Model deployment](#Model-deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-bracelet",
   "metadata": {},
   "source": [
    "### TFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27862572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import \\\n",
    " InteractiveContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc56a3e",
   "metadata": {},
   "source": [
    "Once the requirements are imported, you can create a context object. The\n",
    "context object handles component execution and displays the component’s\n",
    "artifacts. At this point, the InteractiveContext also sets up a simple inmemory ML MetadataStore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f923eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = InteractiveContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ae75d",
   "metadata": {},
   "source": [
    "After setting up your pipeline component(s) (e.g., StatisticsGen), you\n",
    "can then execute each component object through the run function of the\n",
    "context object, as shown in the following example:\n",
    "\n",
    "```\n",
    "from tfx.components import StatisticsGen\n",
    "statistics_gen = StatisticsGen(\n",
    " examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)\n",
    "```\n",
    "\n",
    "The component itself receives the outputs of the previous component (in\n",
    "our case, the data ingestion component ExampleGen) as an instantiation\n",
    "argument. After executing the component’s tasks, the component\n",
    "automatically writes the metadata of the output artifact to the metadata\n",
    "store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-student",
   "metadata": {},
   "source": [
    "### Apache Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-vulnerability",
   "metadata": {},
   "source": [
    "Data pipelines usually start and end with data being read or written, which\n",
    "is handled in Apache Beam through collections, often called\n",
    "PCollections. The collections are then transformed, and the final result\n",
    "can be expressed as a collection again and written to a filesystem.\n",
    "\n",
    "Similar to the ReadFromText operation, Apache Beam provides functions\n",
    "to write collections to a text file (e.g., WriteToText)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f563011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "with beam.Pipeline() as p:\n",
    "    lines = p | beam.io.ReadFromText(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-throw",
   "metadata": {},
   "source": [
    "In Apache Beam, data is manipulated through transformations. As we see in\n",
    "this example and later in Chapter 5, the transformations can be chained by\n",
    "using the pipe operator |. If you chain multiple transformations of the same\n",
    "type, you have to provide a name for the operation, noted by the string\n",
    "identifier between the pipe operator and the right-angle brackets. In the\n",
    "following example, we apply all transformations sequentially on our lines\n",
    "extracted from the text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (\n",
    " lines\n",
    " | 'Split' >> beam.FlatMap(lambda x: re.findall(r'[A-Za-z\\']+', x))\n",
    " | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\n",
    " | 'GroupAndSum' >> beam.CombinePerKey(sum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-morning",
   "metadata": {},
   "source": [
    "You can also apply Python functions as part of a transformation. The\n",
    "following example shows how the function format_result can be applied\n",
    "to earlier produced summation results. The function converts the resulting\n",
    "tuples into a string that then can be written to a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_result(word_count):\n",
    "    \"\"\"Convert tuples (token, count) into a string\"\"\"\n",
    "    (word, count) = word_count\n",
    "    return \"{}: {}\".format(word, count)\n",
    "output = counts | 'Format' >> beam.Map(format_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-kingston",
   "metadata": {},
   "source": [
    "The previous snippets and following\n",
    "examples are a modified version of the Apache Beam introduction. For\n",
    "readability, the example has been reduced to the bare minimum Apache\n",
    "Beam code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "\n",
    "input_file = \"gs://dataflow-samples/shakespeare/kinglear.txt\"\n",
    "output_file = \"/tmp/output.txt\"\n",
    "# Define pipeline options object.\n",
    "pipeline_options = PipelineOptions()\n",
    "\n",
    "\n",
    "with beam.Pipeline(options=pipeline_options) as p:\n",
    "    # Read the text file or file pattern into a PCollection.\n",
    "    lines = p | ReadFromText(input_file)\n",
    "    \n",
    "    # Count the occurrences of each word.\n",
    "    counts = (\n",
    "                lines\n",
    "                | 'Split' >> beam.FlatMap(lambda x: re.findall(r'[A-Za-z\\']+', x))\n",
    "                | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\n",
    "                | 'GroupAndSum' >> beam.CombinePerKey(sum))\n",
    "    \n",
    "    # Format the counts into a PCollection of strings.\n",
    "    \n",
    "    def format_result(word_count):\n",
    "        (word, count) = word_count\n",
    "        return \"{}: {}\".format(word, count)\n",
    "    \n",
    "    output = counts | 'Format' >> beam.Map(format_result)\n",
    "    # Write the output using a \"Write\" transform that has side effects.\n",
    "    output | WriteToText(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-radius",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-minutes",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-contrast",
   "metadata": {},
   "source": [
    "In this step of our pipeline, we read data files or request the data for our\n",
    "pipeline run from an external service (e.g., Google Cloud BigQuery).\n",
    "Before passing the ingested dataset to the next component, we divide the\n",
    "available data into separate datasets (e.g., training and validation datasets)\n",
    "and then convert the datasets into TFRecord files containing the data\n",
    "represented as tf.Example data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(os.pardir, \"data\")\n",
    "examples = external_input(os.path.join(base_dir, data_dir))\n",
    "example_gen = CsvExampleGen(input=examples)\n",
    "\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-jefferson",
   "metadata": {},
   "source": [
    "Sometimes our data can’t be expressed efficiently as CSVs (e.g., when we\n",
    "want to load images for computer vision problems or large corpora for\n",
    "natural language processing problems). In these cases, it is recommended to\n",
    "convert the datasets to TFRecord data structures and then load the saved\n",
    "TFRecord files with the ImportExampleGen component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tfx.components import ImportExampleGen\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(os.pardir, \"tfrecord_data\")\n",
    "examples = external_input(os.path.join(base_dir, data_dir))\n",
    "example_gen = ImportExampleGen(input=examples)\n",
    "\n",
    "context.run(example_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-color",
   "metadata": {},
   "source": [
    "Loading custom files. To convert data of any type to TFRecord files, we need to create a\n",
    "tf.Example structure for every data record in the dataset. tf.Example is a\n",
    "simple but highly flexible data structure, which is a key-value mapping:\n",
    "{\"string\": value}\n",
    "\n",
    "In the case of TFRecord data structure, a tf.Example expects a\n",
    "tf.Features object, which accepts a dictionary of features containing keyvalue pairs. The key is always a string identifier representing the feature\n",
    "column, and the value is a tf.train.Feature object.\n",
    "\n",
    "To reduce code redundancy, we’ll define helper functions to assist with\n",
    "converting the data records into the correct data structure used by\n",
    "tf.Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "original_data_file = os.path.join(\n",
    "                                    os.pardir, os.pardir, \"data\",\n",
    "                                    \"consumer-complaints.csv\")\n",
    "\n",
    "tfrecord_filename = \"consumer-complaints.tfrecord\"\n",
    "tf_record_writer = tf.io.TFRecordWriter(tfrecord_filename)\n",
    "\n",
    "with open(original_data_file) as csv_file:\n",
    "    reader = csv.DictReader(csv_file, delimiter=\",\", quotechar='\"')\n",
    "    for row in reader:\n",
    "        \n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            \"product\": _bytes_feature(row[\"product\"]),\n",
    "            \"sub_product\": _bytes_feature(row[\"sub_product\"]),\n",
    "            \"issue\": _bytes_feature(row[\"issue\"]),\n",
    "            \"sub_issue\": _bytes_feature(row[\"sub_issue\"]),\n",
    "            \"state\": _bytes_feature(row[\"state\"]),\n",
    "            \"zip_code\": _int64_feature(int(float(row[\"zip_code\"]))),\n",
    "            \"company\": _bytes_feature(row[\"company\"]),\n",
    "            \"company_response\": _bytes_feature(row[\"company_response\"]),\n",
    "            \"consumer_complaint_narrative\": \\\n",
    "            _bytes_feature(row[\"consumer_complaint_narrative\"]),\n",
    "            \"timely_response\": _bytes_feature(row[\"timely_response\"]),\n",
    "            \"consumer_disputed\": _bytes_feature(row[\"consumer_disputed\"]),\n",
    "        }))\n",
    "        tf_record_writer.write(example.SerializeToString())\n",
    "    tf_record_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-exception",
   "metadata": {},
   "source": [
    "Remote data files. The ExampleGen component can read files from remote cloud storage\n",
    "buckets like Google Cloud Storage or AWS Simple Storage Service (S3).\n",
    "TFX users can provide the bucket path to the external_input function, as\n",
    "shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = external_input(\"gs://example_compliance_data/\")\n",
    "example_gen = CsvExampleGen(input=examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-monster",
   "metadata": {},
   "source": [
    "Databases. TFX provides two components to ingest datasets directly from databases. In\n",
    "the following sections, we introduce the BigQueryExampleGen component\n",
    "to query data from BigQuery tables and the PrestoExampleGen component\n",
    "to query data from Presto databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud’s BigQuery tables\n",
    "\n",
    "from tfx.components import BigQueryExampleGen\n",
    "query = \"\"\"\n",
    "         SELECT * FROM `<project_id>.<database>.<table_name>`\n",
    "        \"\"\"\n",
    "example_gen = BigQueryExampleGen(query=query)\n",
    "\n",
    "# Presto databases\n",
    "\n",
    "from proto import presto_config_pb2\n",
    "from presto_component.component import PrestoExampleGen\n",
    "\n",
    "query = \"\"\"\n",
    "         SELECT * FROM `<project_id>.<database>.<table_name>`\n",
    "        \"\"\"\n",
    "\n",
    "presto_config = presto_config_pb2.PrestoConnConfig(\n",
    "        host='localhost',\n",
    "        port=8080)\n",
    "\n",
    "example_gen = PrestoExampleGen(presto_config, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-institution",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-modeling",
   "metadata": {},
   "source": [
    "Each of the introduced ExampleGen components allows us to configure\n",
    "input settings (input_config) and output settings (output_config) for our\n",
    "dataset. If we would like to ingest datasets incrementally, we can define a\n",
    "span as the input configuration. At the same time, we can configure how the\n",
    "data should be split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-tutorial",
   "metadata": {},
   "source": [
    "Splitting one dataset into subsets. The following example shows how we can extend our data ingestion by\n",
    "requiring a three-way split: training, evaluation, and test sets with a ratio of\n",
    "6:2:2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import CsvExampleGen\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(os.pardir, \"data\")\n",
    "\n",
    "output = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "    example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=6),\n",
    "    example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2),\n",
    "    example_gen_pb2.SplitConfig.Split(name='test', hash_buckets=2)\n",
    "    ]))\n",
    "\n",
    "examples = external_input(os.path.join(base_dir, data_dir))\n",
    "example_gen = CsvExampleGen(input=examples, output_config=output)\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-transcription",
   "metadata": {},
   "source": [
    "Preserving existing splits. In some situations, we have already generated the subsets of the datasets\n",
    "externally, and we would like to preserve these splits when we ingest the\n",
    "datasets. We can achieve this by providing an input configuration.\n",
    "\n",
    "For the following configuration, let’s assume that our dataset has been split\n",
    "externally and saved in subdirectories:\n",
    "```\n",
    "└── data\n",
    "   ├── train\n",
    "   |     └─ 20k-consumer-complaints-training.csv\n",
    "   ├── eval\n",
    "   │     └─ 4k-consumer-complaints-eval.csv\n",
    "   └── test\n",
    "         └─ 2k-consumer-complaints-test.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(os.pardir, \"data\")\n",
    "\n",
    "input = example_gen_pb2.Input(splits=[\n",
    "    example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n",
    "    example_gen_pb2.Input.Split(name='eval', pattern='eval/*'),\n",
    "    example_gen_pb2.Input.Split(name='test', pattern='test/*')\n",
    "])\n",
    "\n",
    "examples = external_input(os.path.join(base_dir, data_dir))\n",
    "example_gen = CsvExampleGen(input=examples, input_config=input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-navigator",
   "metadata": {},
   "source": [
    "Spanning Datasets. One of the significant use cases for machine learning pipelines is that we\n",
    "can update our machine learning models when new data becomes available.\n",
    "For this scenario, the ExampleGen component allows us to use spans. Think\n",
    "of a span as a snapshot of data. Every hour, day, or week, a batch extract,\n",
    "transform, load (ETL) process could make such a data snapshot and create\n",
    "a new span.\n",
    "\n",
    "```\n",
    "└── data\n",
    "   ├── export0\n",
    "   |     └─ 20k-consumer-complaints-training.csv\n",
    "   ├── export1\n",
    "   │     └─ 4k-consumer-complaints-eval.csv\n",
    "   └── export2\n",
    "         └─ 2k-consumer-complaints-test.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import CsvExampleGen\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(os.pardir, \"data\")\n",
    "\n",
    "input = example_gen_pb2.Input(splits=[\n",
    "     example_gen_pb2.Input.Split(pattern='export{SPAN}/*')\n",
    "])\n",
    "\n",
    "# or\n",
    "\n",
    "input = example_gen_pb2.Input(splits=[\n",
    "    example_gen_pb2.Input.Split(name='train',\n",
    "                                pattern='export{SPAN}/train/*'),\n",
    "    example_gen_pb2.Input.Split(name='eval',\n",
    "                                pattern='export{SPAN}/eval/*')\n",
    "])\n",
    "\n",
    "examples = external_input(os.path.join(base_dir, data_dir))\n",
    "example_gen = CsvExampleGen(input=examples, input_config=input)\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-great",
   "metadata": {},
   "source": [
    "#### Versioning Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-promise",
   "metadata": {},
   "source": [
    "In machine learning pipelines, we want to track the produced models\n",
    "together with the used datasets, which were used to train the machine\n",
    "learning model. To do this, it is useful to version our datasets.\n",
    "\n",
    "Data versioning allows us to track the ingested data in more detail. This\n",
    "means that we not only store the file name and path of the ingested data in\n",
    "the ML MetadataStore (because it’s currently supported by the TFX\n",
    "components) but also that we track more metainformation about the raw\n",
    "dataset, such as a hash of the ingested data. Such version tracking would\n",
    "allow us to verify that the dataset used during the training is still the dataset\n",
    "at a later point in time. Such a feature is critical for end-to-end ML\n",
    "reproducibility.\n",
    "\n",
    "\n",
    "However, such a feature is currently not supported by the TFX ExampleGen\n",
    "component. If you would like to version your datasets, you can use thirdparty data versioning tools and version the data before the datasets are\n",
    "ingested into the pipeline.\n",
    "\n",
    "If you would like to version your datasets, you can use one of the following\n",
    "tools:\n",
    "1. Data Version Control (DVC).\n",
    "DVC is an open source version control system for machine learning\n",
    "projects. It lets you commit hashes of your datasets instead of the entire\n",
    "dataset itself. Therefore, the state of the dataset is tracked (e.g., via\n",
    "git), but the repository isn’t cluttered with the entire dataset.\n",
    "2. Pachyderm.\n",
    "Pachyderm is an open source machine learning platform running on\n",
    "Kubernetes. It originated with the concept of versioning for data (“Git\n",
    "for data”) but has now expanded into an entire data platform, including\n",
    "pipeline orchestration based on data versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-thriller",
   "metadata": {},
   "source": [
    "### Ingestion Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-helping",
   "metadata": {},
   "source": [
    "##### Structured Data\n",
    "Structured data is often stored in a database or on a disk in file format,\n",
    "supporting tabular data. If the data exists in a database, we can either export\n",
    "it to CSVs or consume the data directly with the PrestoExampleGen or the\n",
    "BigQueryExampleGen components (if the services are available).\n",
    "Data available on a disk stored in file formats supporting tabular data\n",
    "should be converted to CSVs and ingested into the pipeline with the\n",
    "CsvExampleGen component. Should the amount of data grow beyond a few\n",
    "hundred megabytes, you should consider converting the data into TFRecord\n",
    "files or store the data with Apache Parquet.\n",
    "##### Text Data for Natural Language Problems\n",
    "Text corpora can snowball to a considerable size. To ingest such datasets\n",
    "efficiently, we recommend converting the datasets to TFRecord or Apache\n",
    "Parquet representations. Using performant data file types allows an efficient\n",
    "and incremental loading of the corpus documents. The ingestion of the\n",
    "corpora from a database is also possible; however, we recommend\n",
    "considering network traffic costs and bottlenecks.\n",
    "##### Image Data for Computer Vision Problems\n",
    "We recommend converting image datasets from the image files to\n",
    "TFRecord files, but not to decode the images. Any decoding of highly\n",
    "compressed images only increases the amount of disk space needed to store\n",
    "the intermediate tf.Example records. The compressed images can be\n",
    "stored in tf.Example records as byte strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "base_path = \"/path/to/images\"\n",
    "filenames = os.listdir(base_path)\n",
    "\n",
    "def generate_label_from_path(image_path):\n",
    "    ...\n",
    "    return label\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "tfrecord_filename = 'data/image_dataset.tfrecord'\n",
    "with tf.io.TFRecordWriter(tfrecord_filename) as writer:\n",
    "    for img_path in filenames:\n",
    "        image_path = os.path.join(base_path, img_path)\n",
    "        try:\n",
    "            raw_file = tf.io.read_file(image_path)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File {} could not be found\".format(image_path))\n",
    "        continue\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'image_raw': _bytes_feature(raw_file.numpy()),\n",
    "                'label': _int64_feature(generate_label_from_path(image_path))\n",
    "                }))\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-archives",
   "metadata": {},
   "source": [
    "The example code reads images from a provided path /path/to/images and\n",
    "stores the image as byte strings in the tf.Example. We aren’t preprocessing\n",
    "our images at this point in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-aerospace",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-prompt",
   "metadata": {},
   "source": [
    "Data validataion mean three\n",
    "distinct checks on our data:\n",
    "* Check for data anomalies.\n",
    "* Check that the data schema hasn’t changed.\n",
    "* Check that the statistics of our new datasets still align with\n",
    "statistics from our previous training datasets.\n",
    "\n",
    "\n",
    "The data validation step in our pipeline performs these checks and\n",
    "highlights any failures. If a failure is detected, we can stop the workflow\n",
    "and address the data issue by hand, for example, by curating a new dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67573dc",
   "metadata": {},
   "source": [
    "Generating Statistics from Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "stats = tfdv.generate_statistics_from_csv(\n",
    "        data_location='/data/consumer_complaints.csv',\n",
    "        delimiter=',')\n",
    "\n",
    "stats = tfdv.generate_statistics_from_tfrecord(\n",
    "        data_location='/data/consumer_complaints.tfrecord')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785de168",
   "metadata": {},
   "source": [
    "Generating Schema from Your Data. Once we have generated our summary statistics, the next step is to generate\n",
    "a schema of our dataset. Data schema are a form of describing the\n",
    "representation of your datasets. A schema defines which features are\n",
    "expected in your dataset and which type each feature is based on (float,\n",
    "integer, bytes, etc.). Besides, your schema should define the boundaries of\n",
    "your data (e.g., outlining minimums, maximums, and thresholds of allowed\n",
    "missing records for a feature).\n",
    "\n",
    "\n",
    "The schema definition of your dataset can then be used to validate future\n",
    "datasets to determine if they are in line with your previous training sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa837060",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8889fe3",
   "metadata": {},
   "source": [
    "Comparing Datasets. Let’s say we have two datasets: training and validation datasets. Before\n",
    "training our machine learning model, we would like to determine how\n",
    "representative the validation set is in regards to the training set. Does the\n",
    "validation data follow our training data schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81acfe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = tfdv.generate_statistics_from_tfrecord(\n",
    "    data_location=train_tfrecord_filename)\n",
    "val_stats = tfdv.generate_statistics_from_tfrecord(\n",
    "    data_location=val_tfrecord_filename)\n",
    "tfdv.visualize_statistics(lhs_statistics=val_stats,\n",
    "    rhs_statistics=train_stats,\n",
    "    lhs_name='VAL_DATASET', rhs_name='TRAIN_DATASET')\n",
    "\n",
    "# Anomalies can be detected using the following code:\n",
    "anomalies = tfdv.validate_statistics(statistics=val_stats, schema=schema)\n",
    "tfdv.display_anomalies(anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585b2d8",
   "metadata": {},
   "source": [
    "Updating the Schema. The preceding anomaly protocol shows us how to detect variations from the\n",
    "schema that is autogenerated from our dataset. But another use case for\n",
    "TFDV is manually setting the schema according to our domain knowledge\n",
    "of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.load_schema_text(schema_location)\n",
    "\n",
    "sub_issue_feature = tfdv.get_feature(schema, 'feature_name')\n",
    "sub_issue_feature.presence.min_fraction = 0.9\n",
    "\n",
    "tfdv.write_schema_text(schema, schema_location)\n",
    "\n",
    "#  revalidate the statistics to view the updated anomalies\n",
    "updated_anomalies = tfdv.validate_statistics(eval_stats, schema)\n",
    "tfdv.display_anomalies(updated_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b636e6",
   "metadata": {},
   "source": [
    "Data Skew and Drift. TFDV provides a built-in “skew comparator” that detects large differences\n",
    "between the statistics of two datasets. This isn’t the statistical definition of\n",
    "skew (a dataset that is asymmetrically distributed around its mean). It is\n",
    "defined in TFDV as the L-infinity norm of the difference between the\n",
    "serving_statistics of two datasets. If the difference between the two\n",
    "datasets exceeds the threshold of the L-infinity norm for a given feature,\n",
    "TFDV highlights it as an anomaly using the anomaly detection defined\n",
    "earlier in this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.get_feature(schema,\n",
    "    'company').skew_comparator.infinity_norm.threshold = 0.01\n",
    "\n",
    "skew_anomalies = tfdv.validate_statistics(statistics=train_stats,\n",
    "    schema=schema,\n",
    "    serving_statistics=serving_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618907f2",
   "metadata": {},
   "source": [
    "Similar to this skew example, you should define your drift_comparator\n",
    "for the features you would like to watch and compare. You can then call\n",
    "validate_statistics with the two dataset statistics as arguments, one for\n",
    "your baseline (e.g., yesterday’s dataset) and one for a comparison (e.g.,\n",
    "today’s dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d43123",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.get_feature(schema,\n",
    "    'company').drift_comparator.infinity_norm.threshold = 0.01\n",
    "drift_anomalies = tfdv.validate_statistics(statistics=train_stats_today,\n",
    "    schema=schema,\n",
    "    previous_statistics=\\\n",
    "    train_stats_yesterday)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82240f",
   "metadata": {},
   "source": [
    "Biased Datasets. Another potential problem with an input dataset is bias. We define bias here\n",
    "as data that is in some way not representative of the real world. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc707f",
   "metadata": {},
   "source": [
    "### Processing Large Datasets with GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58241f47",
   "metadata": {},
   "source": [
    "As we collect more data, the data validation becomes a more timeconsuming step in our machine learning workflow. One way of reducing the\n",
    "time to perform the validation is by taking advantage of available cloud\n",
    "solutions.\n",
    "\n",
    "Set up a pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import (\n",
    " PipelineOptions, GoogleCloudOptions, StandardOptions)\n",
    "\n",
    "\n",
    "options = PipelineOptions()\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = '<YOUR_GCP_PROJECT_ID>'\n",
    "google_cloud_options.job_name = '<YOUR_JOB_NAME>'\n",
    "google_cloud_options.staging_location = 'gs://<YOUR_GCP_BUCKET>/staging'\n",
    "google_cloud_options.temp_location = 'gs://<YOUR_GCP_BUCKET>/tmp'\n",
    "options.view_as(StandardOptions).runner = 'DataflowRunner'\n",
    "\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "setup_options = options.view_as(SetupOptions)\n",
    "setup_options.extra_packages = [\n",
    "                                '/path/to/tensorflow_data_validation'\n",
    "                                '-0.22.0-cp37-cp37m-manylinux2010_x86_64.whl']\n",
    "\n",
    "data_set_path = 'gs://<YOUR_GCP_BUCKET>/train_reviews.tfrecord'\n",
    "output_path = 'gs://<YOUR_GCP_BUCKET>/'\n",
    "tfdv.generate_statistics_from_tfrecord(data_set_path,\n",
    "                                        output_path=output_path,\n",
    "                                        pipeline_options=options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da8e3c",
   "metadata": {},
   "source": [
    "### Integrating TFDV into Your ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db177eb6",
   "metadata": {},
   "source": [
    "TFX provides a pipeline component called StatisticsGen, which accepts\n",
    "the output of the previous ExampleGen components as input and then\n",
    "performs the generation of statistics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0832d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import StatisticsGen\n",
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)\n",
    "\n",
    "from tfx.components import SchemaGen\n",
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=True)\n",
    "context.run(schema_gen)\n",
    "\n",
    "from tfx.components import ExampleValidator\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "context.run(example_validator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c68a7",
   "metadata": {},
   "source": [
    "### Data Preprocessing with TFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18d004",
   "metadata": {},
   "source": [
    "Deploying Preprocessing Steps and the ML Model as\n",
    "One Artifact. To avoid a misalignment between the preprocessing steps and the trained\n",
    "model, the exported model of our pipeline should include the preprocessing\n",
    "graph and the trained model. We can then deploy the model like any other\n",
    "TensorFlow model, but during our inference, the data will be preprocessed\n",
    "on the model server as part of the model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc01eed",
   "metadata": {},
   "source": [
    "TFT processes the data that we ingested into our pipeline with the earlier\n",
    "generated dataset schema, and it outputs two artifacts:\n",
    "* Preprocessed training and evaluation datasets in the TFRecord\n",
    "format. The produced datasets can be consumed downstream in the\n",
    "Trainer component of our pipeline.\n",
    "* Exported preprocessing graph (with assets), which will be used\n",
    "when we’ll export our machine learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b0607",
   "metadata": {},
   "source": [
    "The key to TFT is the preprocessing_fn function.\n",
    "The function defines all transformations we want to apply to the raw data.\n",
    "When we execute the Transform component, the preprocessing_fn\n",
    "function will receive the raw data, apply the transformation, and return the\n",
    "processed data. The data is provided as TensorFlow Tensors or\n",
    "SparseTensors (depending on the feature). All transformations applied to\n",
    "the tensors have to be TensorFlow operations. This allows TFT to\n",
    "effectively distribute the preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    x = inputs['x']\n",
    "    x_normalized = tft.scale_to_0_1(x)\n",
    "    return {\n",
    "    'x_xf': x_normalized\n",
    "    }\n",
    "\n",
    "def process_image(raw_image):\n",
    "    raw_image = tf.reshape(raw_image, [-1])\n",
    "    img_rgb = tf.io.decode_jpeg(raw_image, channels=3)\n",
    "    img_gray = tf.image.rgb_to_grayscale(img_rgb)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    resized_img = tf.image.resize_with_pad(\n",
    "    img,\n",
    "    target_height=300,\n",
    "    target_width=300\n",
    "    )\n",
    "    img_grayscale = tf.image.rgb_to_grayscale(resized_img)\n",
    "    return tf.reshape(img_grayscale, [-1, 300, 300, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe2402",
   "metadata": {},
   "source": [
    "Best Practices:\n",
    "* Feature names matter\n",
    "The naming of the output features of the preprocessing is important. As\n",
    "you will see in the following TFT implementations, we reuse the name\n",
    "of the input feature and append _xf. Also, the names of the input nodes\n",
    "of the TensorFlow models need to match the names of the output\n",
    "features from the preprocessing_fn function.\n",
    "* Consider the data types\n",
    "TFT limits the data types of the output features. It exports all\n",
    "preprocessed features as either tf.string, tf.float32, or tf.int64\n",
    "values. This is important in case your model can’t consume these data\n",
    "types. Some models from TensorFlow Hub require inputs to be\n",
    "presented as tf.int32 values (e.g., BERT models). We can avoid that\n",
    "situation if we cast the inputs to the correct data types inside our models\n",
    "or if we convert the data types in the estimator input functions.\n",
    "* Preprocessing happens in batches\n",
    "When you write preprocessing functions, you might think of it as\n",
    "processing one data row at a time. In fact, TFT performs the operations\n",
    "in batches. This is why we will need to reshape the output of the\n",
    "preprocessing_fn() function to a Tensor or SparseTensor when we\n",
    "use it in the context of our Transform component.\n",
    "* Remember, no eager execution\n",
    "The functions inside of the preprocessing_fn() function need to be\n",
    "represented by TensorFlow ops. If you want to lower an input string,\n",
    "you couldn’t use lower(). You have to use the TensorFlow operation\n",
    "tf.strings.lower() to perform the same procedure in a graph mode.\n",
    "Eager execution isn’t supported; all operations rely on pure TensorFlow\n",
    "graph operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19594ab8",
   "metadata": {},
   "source": [
    "Standalone Execution of TFT. After we have defined our preprocessing_fn function, we need to focus\n",
    "on how to execute the Transform function. For the execution, we have two\n",
    "options. We can either execute the preprocessing transformations in a\n",
    "standalone setup or as part of our machine learning pipeline in the form of a\n",
    "TFX component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55a636",
   "metadata": {},
   "source": [
    "In our example, we would like to apply the normalization preprocessing\n",
    "function that we introduced earlier on our tiny raw dataset, shown in the\n",
    "following source code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50454008",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = [\n",
    "    {'x': 1.20},\n",
    "    {'x': 2.99},\n",
    "    {'x': 100.00}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555885b",
   "metadata": {},
   "source": [
    "First, we need to define a data schema. We can generate a schema from a\n",
    "feature specification, as shown in the following source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    schema_utils.schema_from_feature_spec({\n",
    "    'x': tf.io.FixedLenFeature([], tf.float32),\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742d985",
   "metadata": {},
   "source": [
    "With the dataset loaded and the data schema generated, we can now execute\n",
    "the preprocessing function preprocessing_fn, which we defined earlier. TFT provides bindings for the execution on Apache Beam with the function\n",
    "AnalyzeAndTransformDataset. This function is performing the two-step\n",
    "process we discussed earlier: first analyze the dataset and then transform it.\n",
    "The execution is performed through the Python context manager\n",
    "tft_beam.Context, which allows us to set, for example, the desired batch\n",
    "size. However, we recommend using the default batch size because it is\n",
    "more performant in common use cases. The following example shows the\n",
    "usage of the AnalyzeAndTransformDataset function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4398b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform.beam.impl as tft_beam\n",
    "with beam.Pipeline() as pipeline:\n",
    "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "        tfrecord_file = \"/your/tf_records_file.tfrecord\"\n",
    "        raw_data = (\n",
    "            pipeline | beam.io.ReadFromTFRecord(tfrecord_file))\n",
    "        transformed_dataset, transform_fn = (\n",
    "            (raw_data, raw_data_metadata) |\n",
    "            tft_beam.AnalyzeAndTransformDataset(\n",
    "            preprocessing_fn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88617df2",
   "metadata": {},
   "source": [
    "### Integrate TFT into Your Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2f8b9",
   "metadata": {},
   "source": [
    "In the following code, we define our features. For simpler processing later\n",
    "on, we group the input feature names in dictionaries representing each\n",
    "transform output data type: one-hot encoded features, bucketized features,\n",
    "and raw string representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e32202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "LABEL_KEY = \"consumer_disputed\"\n",
    "# Feature name, feature dimensionality.\n",
    "ONE_HOT_FEATURES = {\n",
    "    \"product\": 11,\n",
    "    \"sub_product\": 45,\n",
    "    \"company_response\": 5,\n",
    "    \"state\": 60,\n",
    "    \"issue\": 90\n",
    "}\n",
    "# Feature name, bucket count.\n",
    "BUCKET_FEATURES = {\n",
    "    \"zip_code\": 10\n",
    "}\n",
    "# Feature name, value is unused.\n",
    "TEXT_FEATURES = {\n",
    "    \"consumer_complaint_narrative\": None\n",
    "}\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "def fill_in_missing(x):\n",
    "    default_value = '' if x.dtype == tf.string or to_string else 0\n",
    "    if type(x) == tf.SparseTensor:\n",
    "        x = tf.sparse.to_dense(\n",
    "        tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "            default_value)\n",
    "    return tf.squeeze(x, axis=1)\n",
    "\n",
    "def convert_num_to_one_hot(label_tensor, num_labels=2):\n",
    "    one_hot_tensor = tf.one_hot(label_tensor, num_labels)\n",
    "    return tf.reshape(one_hot_tensor, [-1, num_labels])\n",
    "\n",
    "def convert_zip_code(zip_code):\n",
    "    if zip_code == '':\n",
    "        zip_code = \"00000\"\n",
    "    zip_code = tf.strings.regex_replace(zip_code, r'X{0,5}', \"0\")\n",
    "    zip_code = tf.strings.to_number(zip_code, out_type=tf.float32)\n",
    "    return zip_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44940a3a",
   "metadata": {},
   "source": [
    "With all the helper functions in place, we can now loop over each feature\n",
    "column and transform it depending on the type. For example, for our\n",
    "features to be converted to one-hot features, we convert the category names\n",
    "to an index with tft.compute_and_apply_vocabulary() and then\n",
    "convert the index to a one-hot vector representation with our helper\n",
    "function convert_num_to_one_hot(). Since we are using\n",
    "tft.compute_and_apply_vocabulary(), TensorFlow Transform will first\n",
    "loop over all categories and then determine a complete category to index\n",
    "mapping. This mapping will then be applied during our evaluation and\n",
    "serving phase of the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    outputs = {}\n",
    "    for key in ONE_HOT_FEATURES.keys():\n",
    "        dim = ONE_HOT_FEATURES[key]\n",
    "        index = tft.compute_and_apply_vocabulary(\n",
    "            fill_in_missing(inputs[key]), top_k=dim + 1)\n",
    "        outputs[transformed_name(key)] = convert_num_to_one_hot(\n",
    "            index, num_labels=dim + 1)\n",
    "    ...\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12faa426",
   "metadata": {},
   "source": [
    "Our processing of the bucket features is very similar. We decided to\n",
    "bucketize the zipcodes because one-hot encoded zip codes seemed too\n",
    "sparse. Each feature is bucketized into, in our case, 10 buckets, and we\n",
    "encode the index of the bucket as one-hot vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eea100",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, bucket_count in BUCKET_FEATURES.items():\n",
    "    temp_feature = tft.bucketize(\n",
    "        convert_zip_code(fill_in_missing(inputs[key])),\n",
    "        bucket_count,\n",
    "        always_return_num_quantiles=False)\n",
    "    outputs[transformed_name(key)] = convert_num_to_one_hot(\n",
    "        temp_feature,\n",
    "        num_labels=bucket_count + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d9d03",
   "metadata": {},
   "source": [
    "Our text input features as well as our label column don’t require any\n",
    "transformations; therefore, we simply convert them to dense features in case\n",
    "a feature might be sparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in TEXT_FEATURES.keys():\n",
    "    outputs[transformed_name(key)] = \\\n",
    "        fill_in_missing(inputs[key])\n",
    "\n",
    "outputs[transformed_name(LABEL_KEY)] = fill_in_missing(inputs[LABEL_KEY])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1456ac",
   "metadata": {},
   "source": [
    "If we use the Transform component from TFX in our pipeline, it expects\n",
    "the transformation code to be provided in a separate Python file. The name\n",
    "of the module file can be set by the user (e.g., in our case module.py), but\n",
    "the entry point preprocessing_fn() needs to be contained in the module\n",
    "file and the function can’t be renamed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb8c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath(\"module.py\"))\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c63b9",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b6130",
   "metadata": {},
   "source": [
    "One very important feature of training a model in a TFX pipeline is that the\n",
    "data preprocessing steps are saved along\n",
    "with the trained model weights. This is incredibly useful once our model is\n",
    "deployed to production because it means that the preprocessing steps will\n",
    "always produce the features the model is expecting. Without this feature, it\n",
    "would be possible to update the data preprocessing steps without updating\n",
    "the model, and then the model would fail in production or the predictions\n",
    "would be based on the wrong data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ea2a6",
   "metadata": {},
   "source": [
    "#### The TFX Trainer Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc1779",
   "metadata": {},
   "source": [
    "run_fn() Function. The Trainer component will look for a run_fn() function in our module\n",
    "file and use the function as an entry point to execute the training process.\n",
    "\n",
    "\n",
    "The run_fn() function is a generic entry point to the training steps and not\n",
    "tf.Keras specific. It carries out the following steps:\n",
    "* Loading the training and validation data (or the data generator)\n",
    "* Defining the model architecture and compiling the model\n",
    "* Training the model\n",
    "* Exporting the model to be evaluated in the next pipeline step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_KEY = 'labels'\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "    return tf.data.TFRecordDataset(filenames,\n",
    "        compression_type='GZIP')\n",
    "\n",
    "def input_fn(file_pattern,\n",
    "            tf_transform_output, batch_size=32):\n",
    "    \n",
    "    transformed_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy())\n",
    "    \n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "                        file_pattern=file_pattern,\n",
    "                        batch_size=batch_size,\n",
    "                        features=transformed_feature_spec,\n",
    "                        reader=_gzip_reader_fn,\n",
    "                        label_key=transformed_name(LABEL_KEY))\n",
    "    return dataset\n",
    "\n",
    "def run_fn(fn_args):\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "    train_dataset = input_fn(fn_args.train_files, tf_transform_output)\n",
    "    eval_dataset = input_fn(fn_args.eval_files, tf_transform_output)\n",
    "    \n",
    "    model = get_model()\n",
    "    \n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps)\n",
    "    \n",
    "    signatures = {\n",
    "        'serving_default':\n",
    "            _get_serve_tf_examples_fn(\n",
    "                model,\n",
    "                tf_transform_output).get_concrete_function(\n",
    "                    tf.TensorSpec(\n",
    "                        shape=[None],\n",
    "                        dtype=tf.string,\n",
    "                        name='examples')\n",
    "                    )\n",
    "    }\n",
    "    model.save(fn_args.serving_model_dir,\n",
    "                save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa0505",
   "metadata": {},
   "source": [
    "The run_fn exports the get_serve_tf_examples_fn as part of the model\n",
    "signature. When a model has been exported and deployed, every prediction\n",
    "request will pass through the serve_tf_examples_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    \n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(\n",
    "            serialized_tf_examples, feature_spec)\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        outputs = model(transformed_features)\n",
    "        return {'outputs': outputs}\n",
    "    \n",
    "    return serve_tf_examples_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4bd33",
   "metadata": {},
   "source": [
    "the Trainer component takes the following as\n",
    "input:\n",
    "The Python module file, here saved as module.py, containing the\n",
    "run_fn(), input_fn(), get_serve_tf_examples_fn(), and\n",
    "other associated functions we discussed earlier\n",
    "The transformed examples generated by the Transform component\n",
    "The transform graph generated by the Transform component\n",
    "The schema generated by the data validation component\n",
    "The number of training and evaluation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Trainer\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.proto import trainer_pb2\n",
    "TRAINING_STEPS = 1000\n",
    "EVALUATION_STEPS = 100\n",
    "trainer = Trainer(\n",
    "    module_file=os.path.abspath(\"module.py\"),\n",
    "    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "    transformed_examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=TRAINING_STEPS),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=EVALUATION_STEPS))\n",
    "\n",
    "context.run(trainer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af6ee4",
   "metadata": {},
   "source": [
    "### Model analysis and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320d21a",
   "metadata": {},
   "source": [
    "TensorFlow Model Analysis. TFMA gives us an easy way to get more detailed metrics than just those\n",
    "used during model training. It lets us visualize metrics as time series across\n",
    "model versions, and it gives us the ability to view metrics on slices of a\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b4bb0",
   "metadata": {},
   "source": [
    "It takes a saved model and an evaluation dataset as input. In this example,\n",
    "we’ll assume a Keras model is saved in SavedModel format and an\n",
    "evaluation dataset is available in the TFRecord file format.\n",
    "First, the SavedModel must be converted to an EvalSharedModel. Next, we provide an EvalConfig. In this step, we tell TFMA what our label\n",
    "is, provide any specifications for slicing the model by one of the features,\n",
    "and stipulate all the metrics we want TFMA to calculate and display. Then, run the model analysis step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f68c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "eval_shared_model = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path=_MODEL_DIR,\n",
    "    tags=[tf.saved_model.SERVING])\n",
    "\n",
    "eval_config=tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "    slicing_specs=[tfma.SlicingSpec()],\n",
    "    metrics_specs=[\n",
    "        tfma.MetricsSpec(metrics=[\n",
    "            tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "            tfma.MetricConfig(class_name='ExampleCount'),\n",
    "            tfma.MetricConfig(class_name='FalsePositives'),\n",
    "            tfma.MetricConfig(class_name='TruePositives'),\n",
    "            tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "            tfma.MetricConfig(class_name='TrueNegatives')\n",
    "        ])\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model,\n",
    "    eval_config=eval_config,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=_EVAL_RESULT_LOCATION,\n",
    "    file_format='tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de4e54",
   "metadata": {},
   "source": [
    "Analysis and Validation in TFX. Up to this point in this chapter, we’ve focused on model analysis with a\n",
    "human in the loop. There are several components in TFX that handle\n",
    "this part of the pipeline: the Resolver, the Evaluator, and the Pusher.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37489ff",
   "metadata": {},
   "source": [
    "ResolverNode. A Resolver component is required if we want to compare a new model\n",
    "against a previous model. ResolverNodes are generic components that\n",
    "query the metadata store. It checks for the last blessed model\n",
    "and returns it as a baseline so it can be passed on to the Evaluator\n",
    "component with the new candidate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92462f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import ResolverNode\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model\n",
    "from tfx.types.standard_artifacts import ModelBlessing\n",
    "model_resolver = ResolverNode(\n",
    "    instance_name='latest_blessed_model_resolver',\n",
    "    resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
    "    model=Channel(type=Model),\n",
    "    model_blessing=Channel(type=ModelBlessing)\n",
    ")\n",
    "context.run(model_resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2574a0b4",
   "metadata": {},
   "source": [
    "Evaluator Component. The Evaluator component uses the TFMA library to evaluate a model’s\n",
    "predictions on a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02182a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "eval_config=tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "    slicing_specs=[tfma.SlicingSpec(),\n",
    "        tfma.SlicingSpec(feature_keys=['product'])],\n",
    "    metrics_specs=[\n",
    "        tfma.MetricsSpec(metrics=[\n",
    "            tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "            tfma.MetricConfig(class_name='ExampleCount'),\n",
    "            tfma.MetricConfig(class_name='AUC')\n",
    "        ])\n",
    "    ]\n",
    ")\n",
    "\n",
    "from tfx.components import Evaluator\n",
    "evaluator = Evaluator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'],\n",
    "    baseline_model=model_resolver.outputs['model'],\n",
    "    eval_config=eval_config\n",
    ")\n",
    "context.run(evaluator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc42b00",
   "metadata": {},
   "source": [
    "TFX Pusher Component. Pusher component is a small but important part of our pipeline. It\n",
    "takes as input a saved model, the output of the Evaluator component, and\n",
    "a file path for the location our models will be stored for serving. It then\n",
    "checks whether the Evaluator has blessed the model (i.e., the model is an\n",
    "improvement on the previous version, and it is above any thresholds we\n",
    "have set). If it has been blessed, the Pusher pushes the model to the serving\n",
    "file path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Pusher\n",
    "from tfx.proto import pusher_pb2\n",
    "_serving_model_dir = \"serving_model_dir\"\n",
    "pusher = Pusher(\n",
    "    model=trainer.outputs['model'],\n",
    "    model_blessing=evaluator.outputs['blessing'],\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory=_serving_model_dir)))\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c865d",
   "metadata": {},
   "source": [
    "### Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8f724",
   "metadata": {},
   "source": [
    "Machine learning models can be deployed in three main ways: with a model\n",
    "server, in a user’s browser, or on an edge device. The most common way\n",
    "today to deploy a machine learning model is with a model server.\n",
    "\n",
    "Most introductions to deploying machine learning models follow roughly\n",
    "the same workflow:\n",
    "* Create a web app with Python (i.e., with web frameworks like\n",
    "Flask or Django).\n",
    "* Create an API endpoint in the web app\n",
    "* Load the model structure and its weights.\n",
    "* Call the predict method on the loaded model.\n",
    "* Return the prediction results as an HTTP request.\n",
    "\n",
    "However, we do not recommend using this strategy to deploy\n",
    "machine learning models to production endpoints because:\n",
    "\n",
    "\n",
    "1. Lack of Code Separation. This means that there\n",
    "would be no separation between the API code and the machine learning\n",
    "model, which can be problematic when data scientists want to update a\n",
    "model and such an update requires coordination with the API team.\n",
    "\n",
    "2. Lack of Model Version Control. If\n",
    "you wanted to add a new version, you would have to create a new endpoint\n",
    "(or add some branching logic to the existing endpoint). This requires extra\n",
    "attention to keep all endpoints structurally the same, and it requires a lot of\n",
    "boilerplate code.\n",
    "\n",
    "3. Inefficient Model Inference. This means each\n",
    "request is preprocessed and inferred individually. The key reason why we\n",
    "argue that such a setup is only for demonstration purposes is that it is highly\n",
    "inefficient. During the training of your model, you will probably use a\n",
    "batching technique that allows you to compute multiple samples at the same\n",
    "time and then apply the gradient change for your batch to your network’s\n",
    "weights. You can apply the same technique when you want the model to\n",
    "make predictions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b1682",
   "metadata": {},
   "source": [
    "TensorFlow Serving. TensorFlow Serving provides you the functionality to load models from a\n",
    "given source (e.g., AWS S3 buckets) and notifies the loader if the source\n",
    "has changed.\n",
    "\n",
    "Before we dive into the TensorFlow Serving configurations, let’s discuss\n",
    "how you can export your machine learning models so that they can be used\n",
    "by TensorFlow Serving.\n",
    "Depending on your type of TensorFlow model, the export steps are slightly\n",
    "different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for keras models\n",
    "saved_model_path = model.save(file path=\"./saved_models\", save_format=\"tf\")\n",
    "\n",
    "# for TensorFlow Estimator\n",
    "import tensorflow as tf\n",
    "def serving_input_receiver_fn():\n",
    "    # an example input feature\n",
    "    input_feature = tf.compat.v1.placeholder(\n",
    "        dtype=tf.string, shape=[None, 1], name=\"input\")\n",
    "    fn = tf.estimator.export.build_raw_serving_input_receiver_fn(\n",
    "        features={\"input_feature\": input_feature})\n",
    "    return fn\n",
    "\n",
    "# Estimator model \n",
    "estimator = tf.estimator.Estimator(model_fn, \"model\", params={})\n",
    "estimator.export_saved_model(\n",
    "    export_dir_base=\"saved_models/\",\n",
    "    serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcead54",
   "metadata": {},
   "source": [
    "TensorFlow Serving\n",
    "allows two different API types: REST and gRPC.\n",
    "\n",
    "* REST API - REST is a communication “protocol” used by today’s web services. REST clients communicate with the server\n",
    "using the standard HTTP methods like GET, POST, DELETE, etc. The\n",
    "payloads of the requests are often encoded as XML or JSON data formats.\n",
    "\n",
    "* gRPC - While gRPC\n",
    "supports different data formats, the standard data format used with gRPC is\n",
    "protocol buffer, which we used throughout this book. gRPC provides lowlatency communication and smaller payloads if protocol buffers are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9527955",
   "metadata": {},
   "source": [
    "#### Making Predictions from the Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80279f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_rest_request(text, model_name=\"my_model\"):\n",
    "    url = \"http://localhost:8501/v1/models/{}:predict\".format(model_name)\n",
    "    payload = {\"instances\": [text]}\n",
    "    response = requests.post(url=url, json=payload)\n",
    "    return response\n",
    "\n",
    "rs_rest = get_rest_request(text=\"classify my text\")\n",
    "rs_rest.json()\n",
    "\n",
    "# --------------------------- OR ----------------------------\n",
    "import grpc\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "import tensorflow as tf\n",
    "def create_grpc_stub(host, port=8500):\n",
    "    hostport = \"{}:{}\".format(host, port)\n",
    "    channel = grpc.insecure_channel(hostport)\n",
    "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "    return stub\n",
    "\n",
    "def grpc_request(stub, data_sample, model_name='my_model', \\\n",
    "    signature_name='classification'):\n",
    "    \n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = model_name\n",
    "    request.model_spec.signature_name = signature_name\n",
    "    request.inputs['inputs'].CopyFrom(tf.make_tensor_proto(data_sample,\n",
    "        shape=[1,1]))\n",
    "    result_future = stub.Predict.future(request, 10)\n",
    "    return result_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e178105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7c537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acec0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
