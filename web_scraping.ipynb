{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "* [Identifying the technology used by a website](#Identifying-the-technology-used-by-a-website)\n",
    "* [Finding the owner of a website](#Finding-the-owner-of-a-website)\n",
    "* [Crawl a website](#crawl-a-website)\n",
    "* [Scraping the data](#Scraping-the-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the technology used by a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web-servers': ['Nginx'],\n",
       " 'web-frameworks': ['Twitter Bootstrap'],\n",
       " 'javascript-frameworks': ['jQuery']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import builtwith\n",
    "builtwith.parse('https://mypage.i-exam.ru/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the owner of a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'domain_name': 'I-EXAM.RU',\n",
       " 'registrar': 'RU-CENTER-RU',\n",
       " 'creation_date': datetime.datetime(2008, 4, 21, 20, 0),\n",
       " 'expiration_date': datetime.datetime(2022, 4, 21, 21, 0),\n",
       " 'name_servers': ['ns3-l2.nic.ru.',\n",
       "  'ns4-cloud.nic.ru.',\n",
       "  'ns4-l2.nic.ru.',\n",
       "  'ns8-cloud.nic.ru.',\n",
       "  'ns8-l2.nic.ru.'],\n",
       " 'status': 'REGISTERED, DELEGATED, VERIFIED',\n",
       " 'emails': None,\n",
       " 'org': '\"Institute of Quality Monitoring Ltd\"'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import whois\n",
    "whois.whois('https://mypage.i-exam.ru/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl a website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scrape a website, we first need to download its web pages containing the\n",
    "data of interestâ€”a process known as crawling. \n",
    "Three common approaches to crawling\n",
    "a website:\n",
    "* Crawling a sitemap\n",
    "* Iterating the database IDs of each web page\n",
    "* Following web page links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a web page\n",
    "import urllib\n",
    "def download(url, user_agent='wswp', num_retries=2):\n",
    "    '''\n",
    "        url - \n",
    "        user_agent - preferable to use an identifiable\n",
    "                     user agent in case problems occur\n",
    "                     with our web crawler. Also, some\n",
    "                     websites block this default user\n",
    "                     agent, perhaps after they experienced\n",
    "                     a poorly made Python web crawler\n",
    "                     overloading their server.\n",
    "                     \n",
    "        num_retries - \n",
    "        \n",
    "    '''\n",
    "    print('Downloading:', url)\n",
    "    \n",
    "    headers = {'User-agent': user_agent}\n",
    "    request = urllib.request.Request(url, headers=headers)\n",
    "    \n",
    "    try:\n",
    "        html = urllib.request.urlopen(request).read().decode('utf-8')\n",
    "    \n",
    "    except urllib.error.URLError as e:\n",
    "        print(f'Download error: {e}')\n",
    "        html = None\n",
    "        \n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "                return download(url, num_retries - 1)\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def crawl_sitemap(url):\n",
    "    # download the sitemap file\n",
    "    sitemap = download(url)\n",
    "    # extract the sitemap links\n",
    "    links = re.findall('<loc>(.*?)</loc>', sitemap) # sitemap parse\n",
    "    # download each link\n",
    "    for link in links:\n",
    "        html = download(link)\n",
    "        # scrape html here\n",
    "        # ...\n",
    "        \n",
    "        \n",
    "# ID iteration crawler\n",
    "import itertools\n",
    "for page in itertools.count(1):\n",
    "    url = 'http://example.webscraping.com/view/-%d' % page\n",
    "    html = download(url)\n",
    "    if html is None:\n",
    "        break\n",
    "    else:\n",
    "        # success - can scrape the result\n",
    "        pass\n",
    "    \n",
    "# Link crawler\n",
    "import urlparse\n",
    "def link_crawler(seed_url, link_regex):\n",
    "    crawl_queue = [seed_url]\n",
    "    # keep track which URL's have seen before\n",
    "    seen = set(crawl_queue)\n",
    "    \n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        html = download(url)\n",
    "        for link in get_links(html):\n",
    "            # check if link matches expected regex\n",
    "            if re.match(link_regex, link):\n",
    "                # form absolute link\n",
    "                link = urlparse.urljoin(seed_url, link)\n",
    "                # check if have already seen this link\n",
    "                if link not in seen:\n",
    "                    seen.add(link)\n",
    "                    crawl_queue.append(link)\n",
    "\n",
    "\n",
    "def get_links(html):\n",
    "    \"\"\"\n",
    "        Return a list of links from html\n",
    "    \"\"\"\n",
    "    # a regular expression to extract all links from the webpage\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']', re.IGNORECASE)\n",
    "    # list of all links from the webpage\n",
    "    return webpage_regex.findall(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://google.com\n"
     ]
    }
   ],
   "source": [
    "http = crawl_sitemap('https://google.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Throttling downloads**.\n",
    "\n",
    "\n",
    "If we crawl a website too fast, we risk being blocked or overloading the server.\n",
    "To minimize these risks, we can throttle our crawl by waiting for a delay between\n",
    "downloads. \n",
    "\n",
    "\n",
    "**Avoiding spider traps**.\n",
    "\n",
    "\n",
    "Some websites dynamically generate their content and can have an infinite number\n",
    "of web pages. For example, if the website has an online calendar with links provided\n",
    "for the next month and year, then the next month will also have links to the next\n",
    "month, and so on for eternity. This situation is known as a spider trap.\n",
    "A simple way to avoid getting stuck in a spider trap is to track how many links\n",
    "have been followed to reach the current web page, which we will refer to as depth.\n",
    "Then, when a maximum depth is reached, the crawler does not add links from this\n",
    "web page to the queue.\n",
    "```\n",
    "def link_crawler(..., max_depth=2):\n",
    "    max_depth = 2\n",
    "    seen = {}\n",
    "    # ...\n",
    "    depth = seen[url]\n",
    "    if depth != max_depth:\n",
    "        for link in links:\n",
    "            if link not in seen:\n",
    "                seen[link] = depth + 1\n",
    "                crawl_queue.append(link)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make this crawler achieve\n",
    "something by extracting data from each web page, which is known as scraping.\n",
    "There are three approaches to scrape a web page:\n",
    "\n",
    "* regular expressions\n",
    "* beautiful soup\n",
    "* Lxml\n",
    "\n",
    "Beautiful Soup is over six times slower than the\n",
    "other two approaches when used to scrape our example web page. This result could\n",
    "be anticipated because lxml and the regular expression module were written in C,\n",
    "while BeautifulSoup is pure Python. An interesting fact is that lxml performed\n",
    "comparatively well with regular expressions, since lxml has the additional overhead\n",
    "of having to parse the input into its internal format before searching for elements. \n",
    "\n",
    "\n",
    "If the bottleneck to your scraper is downloading web pages rather than extracting\n",
    "data, it would not be a problem to use a slower approach, such as Beautiful Soup.\n",
    "Or, if you just need to scrape a small amount of data and want to avoid additional\n",
    "dependencies, regular expressions might be an appropriate choice. However, in\n",
    "general, lxml is the best choice for scraping, because it is fast and robust, while\n",
    "regular expressions and Beautiful Soup are only useful in certain niches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expressions\n",
    "import re\n",
    "url = 'http://example.webscraping.com/view/UnitedKingdom-239'\n",
    "html = download(url)\n",
    "re.findall('<td class=\"w2p_fw\">(.*?)</td>', html)\n",
    "re.findall('<td class=\"w2p_fw\">(.*?)</td>', html)[1]\n",
    "re.findall('<tr id=\"places_area__row\"><td class=\"w2p_fl\"><label for=\"places_area\" \\\n",
    "            id=\"places_area__label\">Area: </label></td><td class=\"w2p_fw\">(.*?)</td>', html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "broken_html = '<ul class=country><li>Area<li>Population</ul>'\n",
    "# parse the HTML\n",
    "soup = BeautifulSoup(broken_html, 'html.parser')\n",
    "fixed_html = soup.prettify()\n",
    "print(fixed_html)\n",
    "'''\n",
    "    <html>\n",
    "         <body>\n",
    "             <ul class=\"country\">\n",
    "                 <li>Area</li>\n",
    "                 <li>Population</li>\n",
    "             </ul>\n",
    "         </body>\n",
    "    </html>\n",
    "'''\n",
    "\n",
    "ul = soup.find('ul', attrs={'class':'country'})\n",
    "ul.find('li') # returns just the first match\n",
    "# <li>Area</li>\n",
    "ul.find_all('li')\n",
    "# [<li>Area</li>, <li>Population</li>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lxml\n",
    "\n",
    "import lxml.html\n",
    "broken_html = '<ul class=country><li>Area<li>Population</ul>'\n",
    "tree = lxml.html.fromstring(broken_html) # parse the HTML\n",
    "fixed_html = lxml.html.tostring(tree, pretty_print=True)\n",
    "\n",
    "'''\n",
    "    we will use CSS selectors here and in future\n",
    "    examples, because they are more compact \n",
    "    and can be reused later in when parsing dynamic conten\n",
    "'''\n",
    "\n",
    "tree = lxml.html.fromstring(html)\n",
    "# This line finds a table row element\n",
    "# with the places_area__row ID, and then selects the child table data tag with the\n",
    "# w2p_fw class.\n",
    "td = tree.cssselect('tr#places_area__row > td.w2p_fw')[0]\n",
    "area = td.text_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSS Selectors.**\n",
    "\n",
    "* Select any tag: *\n",
    "* Select by tag <a></a>: a\n",
    "* Select by class of \"link\": .link\n",
    "* Select by tag <a></a> with class \"link\": a.link\n",
    "* Select by tag <a></a> with ID \"home\": a#home\n",
    "* Select by child <span> of tag <a></a>: a > span\n",
    "* Select by descendant <span> of tag <a></a>: a span\n",
    "* Select by tag <a></a> with attribute title of \"Home\": a[title=Home]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding cache support to the link crawler\n",
    "class Downloader:\n",
    "    def __init__(self, delay=5,\n",
    "        user_agent='wswp', proxies=None,\n",
    "        num_retries=1, cache=None):\n",
    "        self.throttle = Throttle(delay)\n",
    "        self.user_agent = user_agent\n",
    "        self.proxies = proxies\n",
    "        self.num_retries = num_retries\n",
    "        self.cache = cache\n",
    "    \n",
    "    def __call__(self, url):\n",
    "        result = None\n",
    "        \n",
    "        if self.cache:\n",
    "            try:\n",
    "                result = self.cache[url]\n",
    "            except KeyError:\n",
    "                # url is not available in cache\n",
    "                pass\n",
    "            else:\n",
    "                if self.num_retries > 0 and \\\n",
    "                500 <= result['code'] < 600:\n",
    "                # server error so ignore result from cache\n",
    "                # and re-download\n",
    "                result = None\n",
    "                \n",
    "        if result is None:\n",
    "            # result was not loaded from cache\n",
    "            # so still need to download\n",
    "            self.throttle.wait(url)\n",
    "            proxy = random.choice(self.proxies) if self.proxies else None\n",
    "            headers = {'User-agent': self.user_agent}\n",
    "            \n",
    "            result = self.download(url, headers, proxy, self.num_retries)\n",
    "            if self.cache:\n",
    "                # save result to cache\n",
    "                self.cache[url] = result\n",
    "        \n",
    "        return result['html']\n",
    "    \n",
    "    def download(self, url, headers, proxy, num_retries, data=None):\n",
    "        ...\n",
    "        return {'html': html, 'code': code}\n",
    "\n",
    "\n",
    "'''\n",
    "    The link crawler also needs to be slightly updated to support caching by adding the\n",
    "    cache parameter, removing the throttle, and replacing the download function with\n",
    "    the new class\n",
    "'''\n",
    "    \n",
    "def link_crawler(..., cache=None):\n",
    "    crawl_queue = [seed_url]\n",
    "    seen = {seed_url: 0}\n",
    "    num_urls = 0\n",
    "    rp = get_robots(seed_url)\n",
    "    D = Downloader(delay=delay, user_agent=user_agent, proxies=proxies, num_retries=num_retries, cache=cache)\n",
    "    \n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        depth = seen[url]\n",
    "        # check url passes robots.txt restrictions\n",
    "        if rp.can_fetch(user_agent, url):\n",
    "            html = D(url)\n",
    "            links = []\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting part of the Download class used in the preceding code is in the\n",
    "__call__ special method, where the cache is checked before downloading. This\n",
    "method first checks whether the cache is defined. If so, it checks whether this\n",
    "URL was previously cached. If it is cached, it checks whether a server error was\n",
    "encountered in the previous download. Finally, if no server error was encountered,\n",
    "the cached result can be used. If any of these checks fail, the URL needs to be\n",
    "downloaded as usual, and the result will be added to the cache. The download\n",
    "method of this class is the same as the previous download function, except now it\n",
    "returns the HTTP status code along with the downloaded HTML so that error codes\n",
    "can be stored in the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disk cache**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep our file path safe across filesystems, it needs to be restricted to\n",
    "numbers, letters, basic punctuation, and replace all other characters with an\n",
    "underscore, as shown in the following code:\n",
    "\n",
    "```\n",
    ">>> import re\n",
    ">>> url = 'http://example.webscraping.com/default/view/\n",
    " Australia-1'\n",
    ">>> re.sub('[^/0-9a-zA-Z\\-.,;_ ]', '_', url)\n",
    "'http_//example.webscraping.com/default/view/Australia-1'\n",
    "\n",
    "```\n",
    "\n",
    "Additionally, the filename and the parent directories need to be restricted to 255\n",
    "characters (as shown in the following code) to meet the length limitations described\n",
    "in the preceding table:\n",
    "\n",
    "```\n",
    ">>> filename = '/'.join(segment[:255] for segment in\n",
    " filename.split('/'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import urlparse\n",
    "class DiskCache:\n",
    "    def __init__(self, cache_dir='cache'):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def url_to_path(self, url):\n",
    "        \"\"\"Create file system path for this URL\n",
    "        \"\"\"\n",
    "        components = urlparse.urlsplit(url)\n",
    "        # append index.html to empty paths\n",
    "        path = components.path\n",
    "        if not path:\n",
    "            path = '/index.html'\n",
    "        elif path.endswith('/'):\n",
    "            path += 'index.html'\n",
    "        filename = components.netloc + path + components.query\n",
    "        # replace invalid characters\n",
    "        filename = re.sub('[^/0-9a-zA-Z\\-.,;_ ]', '_', filename)\n",
    "        # restrict maximum number of characters\n",
    "        filename = '/'.join(segment[:250] for segment in filename.split('/'))\n",
    "        \n",
    "        return os.path.join(self.cache_dir, filename)\n",
    "    \n",
    "    def __getitem__(self, url):\n",
    "        \"\"\"Load data from disk for this URL\n",
    "        \"\"\"\n",
    "        path = self.url_to_path(url)\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as fp:\n",
    "                return pickle.load(fp)\n",
    "        else:\n",
    "            # URL has not yet been cached\n",
    "            raise KeyError(url + ' does not exist')\n",
    "        \n",
    "    def __setitem__(self, url, result):\n",
    "        \"\"\"Save data to disk for this url\n",
    "        \"\"\"\n",
    "        path = self.url_to_path(url)\n",
    "        folder = os.path.dirname(path)\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        with open(path, 'wb') as fp:\n",
    "            fp.write(pickle.dumps(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database cache**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the anticipated limitations to our disk-based cache, we will now build\n",
    "our cache on top of an existing database system. When crawling, we may need to\n",
    "cache massive amounts of data and will not need any complex joins, so we will use\n",
    "a NoSQL database, which is easier to scale than a traditional relational database.\n",
    "Specifically, our cache will use MongoDB, which is currently the most popular\n",
    "NoSQL database.\n",
    "\n",
    "\n",
    "NoSQL stands for Not Only SQL and is a relatively new approach to database\n",
    "design. The traditional relational model used a fixed schema and splits the data into\n",
    "tables. However, with large datasets, the data is too big for a single server and needs\n",
    "to be scaled across multiple servers. This does not fit well with the relational model\n",
    "because, when querying multiple tables, the data will not necessarily be available on\n",
    "the same server. NoSQL databases, on the other hand, are generally schemaless and\n",
    "designed from the start to shard seamlessly across servers. There have been multiple\n",
    "approaches to achieve this that fit under the NoSQL umbrella. There are column data\n",
    "stores, such as HBase; key-value stores, such as Redis; document-oriented databases,\n",
    "such as MongoDB; and graph databases, such as Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient\n",
    "class MongoCache:\n",
    "    def __init__(self, client=None, expires=timedelta(days=30)):\n",
    "        # if a client object is not passed then try\n",
    "        # connecting to mongodb at the default localhost port\n",
    "        self.client = MongoClient('localhost', 27017) if client is None else client\n",
    "        # create collection to store cached webpages,\n",
    "        # which is the equivalent of a table\n",
    "        # in a relational database\n",
    "        self.db = client.cache\n",
    "        # create index to expire cached webpages\n",
    "        self.db.webpage.create_index('timestamp',\n",
    "            expireAfterSeconds=expires.total_seconds())\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, url):\n",
    "        \"\"\"Load value at this URL\n",
    "        \"\"\"\n",
    "        record = self.db.webpage.find_one({'_id': url})\n",
    "        if record:\n",
    "            return record['result']\n",
    "        else:\n",
    "            raise KeyError(url + ' does not exist')\n",
    "            \n",
    "    def __setitem__(self, url, result):\n",
    "        \"\"\"Save value for this URL\n",
    "        \"\"\"\n",
    "        record = {'result': result, 'timestamp':\n",
    "            datetime.utcnow()}\n",
    "        self.db.webpage.update({'_id': url}, {'$set': record},\n",
    "            upsert=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
